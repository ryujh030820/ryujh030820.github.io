---
published: true
title: "[LG Aimers] [지도학습-6] More On Supervised Learning Beyond"
date: 2025-01-12 14:48:00 +0900
categories: [Lecture, LG Aimers]
tags: [lg aimers, ai]
math: true
---
## 딥러닝이 항상 필요하지 않은 경우

- **간단한 문제일 경우**: 모델도 단순해야 함.
    - 복잡한 딥러닝 모델은 과잉 설계(overkill)일 수 있음.

## 나이브 베이즈 (Naive Bayes)

- **응용 사례**: 스팸 필터링.
    - 이메일을 특정 단어가 포함되었는지에 따라 벡터로 표현.
- **나이브 베이즈 가정**:
    - X 변수들은 주어진 y에 대해 독립적.
- **라플라스 스무딩 (Laplace Smoothing)**:
    - 등장하지 않은 단어도 모델에 반영하기 위해 빈도를 1로 시작.

## 결정 트리 (Decision Tree)

- **구조**:
    - **내부 노드**: 특정 결정 기준.
    - **브랜치**: 결정 결과.
    - **리프 노드**: 최종 클래스 레이블(분류) 또는 값(회귀).
- **분류 예시**:
    - 동물이 포유류인지 아닌지 판단.
        - "털이 있는지?" → "새끼를 낳는지?"
- **회귀 예시**:
    - 집값 예측.
        - "평수가 2000 이상인가?" → "방 개수가 3개 이상인가?"
- **분할 기준**:
    - 분류: 엔트로피(Entropy)로 산포도 측정.
    - 회귀: 분산(Variance) 측정.

## 배깅 (Bagging)

- **Bootstrap Aggregating**:
    - 학습 데이터의 일부를 중복 샘플링하여 다수의 서브셋 생성.
- **특징**:
    - 과적합(overfitting) 감소.
    - 모델의 강건성(robustness) 향상.

## 랜덤 포레스트 (Random Forest)

- **앙상블 방법**:
    - 여러 결정 트리를 조합하여 예측.

## 부스팅 (Boosting)

- **특징**:
    - **순차적 학습**:
        - 초기 모델 학습 후, 오분류된 샘플에 가중치 부여.
        - 이후 가중치 기반 학습 반복.
    - 최종적으로 각 모델의 가중치 합산.
- **AdaBoost**:
    - 가중치는 에러율에 기반.

## 배깅 vs 부스팅

| **배깅** | **부스팅** |
| --- | --- |
| 병렬 처리 가능 | 순차 처리 필요 |
| 분산 감소 | 편향 감소 |
| 투표(분류) / 평균(회귀) | 가중 투표 / 가중 평균 |

## 지도학습 응용 사례

- **Super Resolution**:
    - 저해상도/고해상도 이미지 데이터 사용.
    - 모델: CNN.
    - 손실 함수: 픽셀 단위 MSE 또는 PSNR.
- **물체 탐지**:
    - 데이터: 이미지, 박스, 레이블.
    - 모델: Faster R-CNN.
    - 손실 함수: 박스 좌표 차이 + 분류 손실.
- **BERT**:
    - 데이터: 마스킹된 문장, 토큰화된 문장.
    - 모델: Transformer.
    - 손실 함수: 분류 손실.
- **이상 탐지**:
    - 데이터: 센서 입력, 이상 지표.
    - 모델: 딥러닝 모델.
    - 손실 함수: 가중치 기반 분류 손실.

## 지도학습을 넘어

- **반지도 학습 (Semi-Supervised Learning)**:
    - 레이블링이 비싼 문제 해결.
    - 데이터 증강(augmentation)을 통해 비지도 데이터 활용.
- **생성 모델**:
    - 라벨 없이 데이터 분포 \( p(x) \) 학습.
    - 예: GAN, Diffusion Models.
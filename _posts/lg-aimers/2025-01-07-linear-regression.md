---
published: true
title: "[LG Aimers] [지도학습-2] Linear Regression"
date: 2025-01-07 18:27:00 +0900
categories: [Lecture, LG Aimers]
tags: [lg aimers, ai]
math: true
---
## 선형 회귀의 기본 개념

- **목표:** $y = f(x)$라는 참 함수를 모델링하는 것.
    - 데이터셋 ($x_i, y_i$)를 기반으로 모델 $\hat{f}(x)$를 찾음.
    - f(x)는 주어진 데이터와 유사한 값(예측)을 생성해야 함.

## 단일 입력 선형 회귀

- **데이터:** 키(\(x\))와 몸무게(\(y\)).
- **함수 클래스 (모델):**
$\hat{y} = \theta_0 + \theta_1 x$
- **손실 함수:**
    - **평균 제곱 오차(MSE):**
    $L(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
    - **MSE를 사용하는 이유:**
        - 미분 가능성.
        - 분석적 해가 존재함.

## 다변량 선형 회귀

- **데이터:** $(x_1, x_2, \dots, x_n)$와 $y$.
- **모델 클래스:**
$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$
- **손실 함수:** 다차원 \(MSE\) 동일.
- **최소화 문제:**
$\min_\theta L(\theta)$
    - 기울기를 0으로 만드는 해를 계산.

## 정규 방정식 (Normal Equation)

- **선형 회귀의 해:**
$\theta = (X^T X)^{-1} X^T y$
    - $X$: 특성 행렬.
    - $y$: 실제 값 벡터.
- **장점:**
    - 계산적으로 간단.
    - 소규모 데이터셋에 적합.

## 고차 방정식으로 확장

- **예:** 2차 방정식 모델 $y = ax^2 + bx + c$를 사용하고 싶을 때.
    - **데이터셋 재정의:**
    $X = [1, x, x^2]$
    - **모델 클래스:**
    $\hat{y} = \theta_0 + \theta_1 x + \theta_2 x^2$
    - **손실 함수:** 여전히 $MSE$.

## 과적합(Overfitting)과 편향-분산 트레이드오프

- **과적합:**
    - 모델이 훈련 데이터에 너무 적합하여 일반화 실패.
- **편향-분산 트레이드오프:**
    - 단순한 모델 → 높은 편향, 낮은 분산.
    - 복잡한 모델 → 낮은 편향, 높은 분산.
- **해결책:**
    - 더 많은 데이터 확보.
    - 정규화 (Regularization): $L2$ 항 추가.

## 모델 검증

- **훈련 데이터와 검증 데이터로 분리.**
    - 훈련 손실과 검증 손실을 비교하여 과적합 여부 확인.
    - **훈련 손실 ≈ 검증 손실:** 일반화 잘됨.
    - **훈련 손실 ≪ 검증 손실:** 과적합.
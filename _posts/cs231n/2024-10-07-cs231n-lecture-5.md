---
published: true
title: "[CS231n 5강] Convolutional Neural Networks"
date: 2024-10-07 15:30:00 +0900
categories: [Lecture, CS231N]
tags: [cs231n, computer vision]
math: true
---
## Convolutional Neural Networks

![Untitled.png](/assets/img/cs231n-lecture-5/Untitled.png)

오늘은 `CNN(합성곱 신경망)`에 대해 알아볼 것이다. 

Convolutional layer의 가장 큰 특징이자, Fully connected layer와 구분되는 특징은 기본적으로 **“공간적 구조”**를 유지한다는 사실이다.

## A bit of history

### Hubel & Wiesel의 연구

![Untitled 1.png](/assets/img/cs231n-lecture-5/Untitled 1.png)

![Untitled 2.png](/assets/img/cs231n-lecture-5/Untitled 2.png)

다시 1950년대로 돌아가 보면, Hubel과 Wiesel은 고양이의 뇌에 전극을 꽂아 일차 시각 피질의 뉴런에 관한 연구를 수행했다.

이 실험에서 뉴런이 oriented edges와 shapes 같은 것에 반응한다는 것을 알아냈다.

그리고 이 실험에서 내린 몇 가지 결론은 아주 중요했는데, 첫번째로 발견한 피질 내 서로 인접해 있는 세포들은 어떤 지역성을 띠고 있었다. 위 사진의 오른쪽 그림은 해당하는 spatial mapping을 볼 수 있다.

또한 이 실험에서 뉴런들이 계층 구조를 지닌다는 것도 발견했다. 상위의 cell들은 보다 단순한 구조들을 감지하고, 하위의 cell들은 좀 더 복잡한 구조들을 감지한다.

## Convolution Layer

![Untitled 11.png](/assets/img/cs231n-lecture-5/Untitled 11.png)

기존의 FC Layer가 입력 이미지를 길게 쭉 폈다면 이제는 기존의 이미지 구조를 그대로 유지하게 된다.

위 예시에서는 5x5x3 필터가 이미지를 슬라이딩 하면서 공간적으로 내적을 수행하게 된다.

![Untitled 13.png](/assets/img/cs231n-lecture-5/Untitled 13.png)

![Untitled 14.png](/assets/img/cs231n-lecture-5/Untitled 14.png)

이런 식으로 여러 개의 필터들을 사용해, 원하는 개수 만큼의 activation map들을 만들 수 있다.

![Untitled 15.png](/assets/img/cs231n-lecture-5/Untitled 15.png)

위 사진과 같이, Conv layer들과 Activation function들을 번갈아 가며 쌓는 식으로 우리는 CNN을 구성할 수 있다. 중간에 Pooling layer들이 들어갈 수도 있다.

![Untitled 16.png](/assets/img/cs231n-lecture-5/Untitled 16.png)

feature들을 시각화한 모습이다. 앞서 살펴본 Hubel과 Wiesel의 이론과 같이, 하위 계층으로 갈수록 더 자세한 영역까지 다루는 것을 확인할 수 있다.

![Untitled 17.png](/assets/img/cs231n-lecture-5/Untitled 17.png)

CNN이 어떻게 수행되는지를 살펴보면, 입력 이미지는 여러 레이어들을 통과해 최종적으로는 Conv 출력 모두와 연결되어 있는 FC Layer을 이용해 최종 스코어를 계산한다.

## A closer look at spatial dimension

![Untitled 18.png](/assets/img/cs231n-lecture-5/Untitled 18.png)

Activation map이 생성되는 과정을 상세히 살펴보자면, 필터를 이미지의 왼상단에서 시작해 stride만큼 이동하여 해당 값들의 내적을 수행한다.

![Untitled 22.png](/assets/img/cs231n-lecture-5/Untitled 22.png)

위의 예제의 경우 7x7 input에 3x3 filter를 stride 1만큼 씩 움직이므로, 5x5 output activation map을 얻게 될 것이다.

![Untitled 24.png](/assets/img/cs231n-lecture-5/Untitled 24.png)

![Untitled 25.png](/assets/img/cs231n-lecture-5/Untitled 25.png)

그러나 stride가 3인 경우 이미지가 딱 떨어지지 않는다. 모서리 부분은 어쩔 수 없이 일부 이미지가 비게 된다. 이때 padding 기법을 사용하게 된다.

## Zero Padding

![Untitled 27.png](/assets/img/cs231n-lecture-5/Untitled 27.png)

Zero Padding이란 양쪽 사이드에 zero 값을 둘러주어 이미지의 크기를 늘리는 것이다.

위와 같이 이미지가 딱 떨어지지 않는 상황이나, 갈수록 activation map의 크기가 작아지는 현상을 방지하기 위해서 사용한다.

## Pooling layer

![Untitled 37.png](/assets/img/cs231n-lecture-5/Untitled 37.png)

![Untitled 38.png](/assets/img/cs231n-lecture-5/Untitled 38.png)

Pooling layer는 이미지의 크기를 줄이는 역할을 하며, 대표적으로 Max Pooling 기법을 많이 사용한다.
---
published: true
title: "[CS231n 6강] Training Neural Networks I"
date: 2024-10-30 10:50:00 +0900
categories: [Lecture, CS231N]
tags: [cs231n, ai]
math: true
---
## Activation Function이란?

![1.png](/assets/img/cs231n-lecture-6/1.png)

Neural Network의 모습을 그림으로 표현하면 위 사진과 같다. 각 뉴런에서 들어온 입력값 $x_i$를 $w_i$와 곱한 것을 모두 합하고, 바이어스(bias)를 더한 다음 비선형 연산인 **활성화 함수(Activation Function)**를 거친다.

## Sigmoid 함수

![2.png](/assets/img/cs231n-lecture-6/2.png)

**Sigmoid 함수**는 위 사진과 같은 형태를 띄며, 각 입력을 받아서 그 입력을 [0, 1] 사이의 값이 되도록 한다.

뉴런의 “firing rate”를 포화시키는 것으로 해석할 수 있기 때문에 역사적으로 유명했었으나, 나중에 ReLU와 같은 활성화 함수가 생물학적 타당성이 더 크다는게 밝혀졌다.

Sigmoid 함수는 다음과 같은 문제점들을 갖고 있다.

1. **포화된 뉴런이 그래디언트를 죽인다.**
2. **Sigmoid 함수의 출력값이 0을 중심으로 하지 않는다.**
3. **exp()로 인해 계산 비용이 크다. (이 문제는 그렇게 큰 문제가 아니다.)**
    
    ![3.png](/assets/img/cs231n-lecture-6/3.png)
    

첫 번째 문제점에 대해서는 Sigmoid 함수가 0과 1에 가까운 값일 경우, gradient가 0이 되기 때문에 **거의 0에 가까운 값이 backprob** 된다는 것이다.

![4.png](/assets/img/cs231n-lecture-6/4.png)

두 번째 문제점이 문제인 이유는 뉴런의 입력이 항상 양수일 때, W에 대한 gradient 값이 모두 양수이거나 음수가 되기 때문이다.

이렇게 되면 gradient가 이동할 수 있는 방향은 4분면 중 두 영역뿐이 안 되기 때문에, **gradient update가 비효율적**이게 된다.

## tanh(x) 함수

![5.png](/assets/img/cs231n-lecture-6/5.png)

두 번째 활성화 함수로는 **tanh(x) 함수**가 있다. Sigmoid와 유사한 형태를 띄지만 [-1, 1] 사이의 범위를 갖는다.

Sigmoid 함수와 가장 큰 차이점이 있다면 **zero-centered**라는 점이다.

하지만 입력값이 굉장히 큰 양수나, 굉장히 작은 음수일 경우 여전히 saturation 때문에 gradient가 죽는 문제가 발생한다.

## ReLU 함수

![6.png](/assets/img/cs231n-lecture-6/6.png)

이번에 살펴볼 **ReLU 함수**는 $f(x)=max(0,x)$로 표현되며, 앞서 살펴본 문제점을 해결한 활성화 함수다.

적어도 입력 스페이스의 절반은 saturation 되지 않으며, 단순한 max 연산이므로 **계산이 매우 빠르다**.

Sigmoid 함수나 tanh 함수보다 **수렴 속도가 6배 가량 더 빠르며**, **생물학적 타당성도 ReLU가 sigmoid보다 더 크다**.

그러나 ReLU는 문제점을 안고 있는데,

1. **ReLU 함수는 zero-centered가 아니다.**
2. **양의 수에서는 saturation 되지 않지만 음의 경우에서는 그렇지 않다.**
    
    ![7.png](/assets/img/cs231n-lecture-6/7.png)
    

Gradient의 절반을 죽이는 **dead ReLU 현상**을 위 사진을 통해 이해할 수 있는데, data cloud는 training data이고 초록색 선과 빨간색 선(평면)이 각 ReLU를 의미한다.

Dead ReLU에서는 activate가 일어나지 않고 update되지 않는다. 반면 active ReLU는 일부는 active되고 일부는 active되지 않는다.

몇 가지 이유로 이런 일이 발생할 수 있는데,

1. **위 사진의 dead ReLU처럼 가중치 평면이 data cloud에서 멀리 떨어져 있는 경우**
2. **Learning rate가 지나치게 높은 경우**

가 있다.

## Leaky ReLU / PReLU

![8.png](/assets/img/cs231n-lecture-6/8.png)

ReLU의 문제점을 해결하기 위해 **Leaky ReLU**와 **PReLU**가 나왔다.

Leaky ReLU와 PReLU의 장점은 어느 곳에서도 그래프가 죽지 않는다는 것이다.

## ELU

![9.png](/assets/img/cs231n-lecture-6/9.png)

**ELU**는 ReLU와 Leaky ReLU의 중간으로, zero-mean에 가까운 출력값을 갖지만 음수에서 기울기를 가지는 것 대신에 또 다시 saturation된다.

ELU에서는 이런 saturation이 좀 더 노이즈에 강인할 수 있다고 주장한다.

## Maxout Neuron

![10.png](/assets/img/cs231n-lecture-6/10.png)

**Maxout Neuron**은 ReLU와 Leaky ReLU의 일반화로 볼 수 있으며, 포화되지 않고, 뉴런이 “죽는”(Die) 현상이 발생하지 않는다.

그러나 뉴런당 파라미터가 2배가 된다는 단점이 있다.

## TLDR: In practice:

![11.png](/assets/img/cs231n-lecture-6/11.png)

실제로 가장 많이 쓰이는 것은 ReLU이며, 다만 learning rate를 아주 조심스럽게 결정해야 한다.

또한 Leaky ReLU, Maxout, ELU 같은 것들도 사용해볼 수 있다.

Sigmoid는 사용하지 않는 편이 좋다.

## Data Preprocessing

![12.png](/assets/img/cs231n-lecture-6/12.png)

가장 대표적인 전처리 과정은 **zero-mean**으로 만들고 **normalize(정규화)**를 하는 것이다.

이를 해주는 이유는 zero-mean 같은 경우 앞서 살펴본 이유와 같고, 정규화 같은 경우 모든 차원이 동일한 범위 안에 있게 해주어 전부 동등한 기여를 하도록 만들기 위함이다.

![13.png](/assets/img/cs231n-lecture-6/13.png)

**이미지 분석에서는 정규화는 할 필요 없이 zero-centered 데이터로만 만들면 된다.** 이미지에서는 각 차원이 이미 특정 범위(0~255) 안에 들어있기 때문이다.

일반적으로 training 데이터에서 계산한 평균을 test 데이터에도 동일하게 적용하며, 일부 네트워크는 채널 전체의 평균을 구하지 않고 채널마다 평균을 독립적으로 계산하기도 한다.

## Weight Initialization

![14.png](/assets/img/cs231n-lecture-6/14.png)

모든 가중치를 0으로 초기화한다면 어떤 일이 발생할까? 정답은 모든 뉴런이 같은 일을 하게 될 것이다.

결국 모든 가중치가 똑같은 값으로 업데이트 되므로, 이는 적절치 못하다.

![15.png](/assets/img/cs231n-lecture-6/15.png)

![16.png](/assets/img/cs231n-lecture-6/16.png)

초기화 문제를 해결하기 위한 첫번째 방법으로 임의의 작은 값으로 초기화하는 것을 생각해볼 수 있다.

위 사진의 경우 표준편차를 0.01로 설정해 초기화하였다.

하지만 이 방법은 더 깊은 네트워크에서 문제가 생길 수 있다. W가 너무 작은 값들이기 때문에 출력 값이 급격하게 줄어들게 되므로 결국 0이 되는 문제가 발생한다.

Backward pass를 생각해 본다면 각 레이어의 입력이 0에 수렴하게 되고, 따라서 local gradient가 0이 되기 때문에 업데이트가 잘 되지 않을 것이다.

![17.png](/assets/img/cs231n-lecture-6/17.png)

그렇다면 가중치가 좀 더 큰 분산을 가지도록 초기화하면 어떨지에 대해 한 번 생각해볼 수 있을 것이다.

해당 가중치를 통과한 WX는 매우 작거나 큰 값을 가질 것이고, 따라서 결과값이 포화됨으로써 gradient는 0이 되어 가중치 업데이트가 일어나지 않을 것이다.

![18.png](/assets/img/cs231n-lecture-6/18.png)

합리적인 가중치를 얻을 수 있는 좋은 방법 중 하나는 2010년에 발표한 **Xavier initialization**이 있다.

가우시안 표준 정규 분포에서 랜덤으로 뽑은 값을 입력의 수로 스케일링 해주는 방식으로 작동하는데, 입력의 수가 작으면 더 작은 값으로 나누어 좀 더 큰 가중치를 얻게 된다.

이는 작은 입력의 수가 가중치와 곱해지기 때문에, 가중치가 더 커야만 출력의 분산만큼 큰 값을 얻을 수 있기 때문이다.

그러나 ReLU를 사용할 때 위 방식은 적절치 않은데, ReLU는 출력의 절반을 죽이는 효과가 있기 때문에 이전과 같은 가정을 해버리면 잘 작동하지 않는다.

![19.png](/assets/img/cs231n-lecture-6/19.png)

이 문제를 해결하기 위한 **He initialization**은 뉴런들 중 절반이 없어진다는 사실을 고려하기 위해서 입력의 수를 2로 나눠준다.

실제로 이 작은 변화는 트레이닝에 있어서 엄청난 차이를 보인다.

## Batch Normalization

![20.png](/assets/img/cs231n-lecture-6/20.png)

**Batch Normalization**은 강제로 레이어의 출력을 unit gaussian으로 만들어 준다.

Batch 단위로 각 레이어에 입력으로 들어오는 모든 값들을 이용해 평균과 분산을 구해서 normalization해주는 것이다.

![21.png](/assets/img/cs231n-lecture-6/21.png)

BN layer는 주로 FC layer나 Convolutional layer의 뒤에 위치시킨다.

Conv layer에서 차이점이 있다면 Normalization을 차원마다 독립적으로 수행하는 것이 아니라 같은 Activation map의 같은 채널에 있는 요소들은 같이 Normalize 해준다는 것이다.

![22.png](/assets/img/cs231n-lecture-6/22.png)

그러나 FC layer나 Convolutional layer를 거칠 때마다 매번 Normalization을 해주는 것에 대한 의문이 있다.

Normalization이 하는 일은 입력이 tanh의 linear한 영역에만 존재하도록 강제하는 것이다.

그러나 saturation이 전혀 일어나지 않는 것보다는 “얼마나” saturation이 일어날지를 우리가 조절하는 것이 더 유연할 것이다.

그래서 우리는 $\gamma$를 통해 scaling하고, $\beta$로는 이동의 효과를 준다.

![23.png](/assets/img/cs231n-lecture-6/23.png)

BN의 장점은 다음과 같다.

- Gradient의 흐름을 원활하게 해주어 학습이 더 잘되게 해준다.
- Learning rate를 증가시킬 수 있다.
- 가중치 초기화에 의지하지 않아도 된다.
- BN은 regularization의 역할도 한다.

## Babysitting the Learning Process

지금까지는 네트워크 설계를 배웠다. 이제는 학습 과정을 어떻게 모니터링하고 하이퍼파라미터를 조절할 것인지를 배워볼 것이다.

### 1. 데이터 전처리

![24.png](/assets/img/cs231n-lecture-6/24.png)

Zero-mean을 사용해 데이터를 전처리한다.

### 2. 아키텍처 선택

![25.png](/assets/img/cs231n-lecture-6/25.png)

그리고 아키텍처를 선택한다. 위 사진에서는 하나의 히든 레이어와 50개의 뉴런을 가진 모델을 사용하였다.

### 3. Sanity check

![26.png](/assets/img/cs231n-lecture-6/26.png)

네트워크를 초기화한 후, forward pass하여 loss를 구한다. 여기서 regularization은 0으로 준다.

네트워크가 잘 동작한다면 그 결과값인 loss는 softmax classifier를 사용할 시 $-log(\frac1N)$이 되어야 한다.

![27.png](/assets/img/cs231n-lecture-6/27.png)

초기 loss가 정상이라는 것을 확인했다면, regularization(1e3)을 추가해 loss 값을 확인해본다.

손실 함수에 regularization term이 추가되기 때문에 당연히 loss 값은 증가할 것이다.

### 4. 학습 (sanity check)

![28.png](/assets/img/cs231n-lecture-6/28.png)

학습을 진행할 준비가 끝났으면 이제 학습을 시작한다.

이때는 regularization을 사용하지 않고 loss가 줄어드는지만 확인한다.

### 5. Learning rate 찾기

![29.png](/assets/img/cs231n-lecture-6/29.png)

가장 먼저 찾아야 하는 하이퍼파라미터는 learning rate이다.

만약 loss가 감소하지 않는다면 이는 learning rate가 너무 낮음을 뜻하고, loss가 증가한다면 이는 learning rate가 너무 높음을 뜻한다. (즉, 발산한다.)

## Hyperparameter Optimization

### Cross-validation

![30.png](/assets/img/cs231n-lecture-6/30.png)

**Cross-validation**은 training set으로 학습시키고, validation set으로 평가해, 주어진 hyperparameter가 잘 동작하는지 확인하는 방법이다.

이를 좀 더 단계적으로 나누면 **coarse stage**와 **fine stage**로 나눌 수 있는데, coarse stage에서는 넓은 범위에서 값을 고르고, fine stage에서는 좀 더 좁은 범위를 설정하고 학습을 길게 시키면서 최적의 값을 얻는다.

## Random Search vs. Grid Search

![31.png](/assets/img/cs231n-lecture-6/31.png)

좋은 하이퍼파라미터를 찾는 다른 방법은 **grid search**를 이용하는 것이다.

하지만 **random search**를 하는 것이 결과가 더 좋은데, 바로 모델이 특정 파라미터의 변화에 더 민감하게 반응을 하고 있다고 했을 때, (노랑 < 초록) random search는 중요한 파라미터(초록)에 더 많은 샘플링이 가능하기 때문이다.
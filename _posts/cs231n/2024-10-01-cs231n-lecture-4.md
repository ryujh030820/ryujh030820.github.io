---
published: true
title: "[CS231n 4강] Introduction to Neural Networks"
author:
  name: JH
  link: https://github.com/ryujh030820
date: 2024-10-02 08:28:00 +0900
categories: [Lecture, CS231N]
tags: [cs231n, computer vision]
math: true
---
## Backpropagation에 대해서

이전 강의에서 어떻게 Gradient Descent를 사용해서 최적화를 시킬 수 있는지에 대해 다루었다.

이번 강의에서는 임의의 복잡한 함수의 Analytic gradient를 계산하는 방법인 `역전파(Backpropagation)`에 대해 다룬다.

Backpropagation은 gradient를 얻기 위해 **computational graph 내부의 모든 변수에 대해 chain rule을 재귀적으로 적용**한다.

## Backpropagation: a simple example

![다운로드.png](/assets/img/cs231n-lecture-4/다운로드.png)

Backpropagation의 가장 간단한 예제는 위와 같다.

f에 대한 q의 미분값과 z의 미분값을 구하고, 이를 이용해 f에 대한 x의 미분값과 y의 미분값을 구한다.

![다운로드 (1).png](/assets/img/cs231n-lecture-4/다운로드 (1).png)

이를 일반화하면 다음과 같이 나타낼 수 있다.

local gradient를 구한 후, L에 대한 상류 노드의 gradient 값을 구하면 최종적으로 L에 대한 입력 노드의 gradient 값을 구할 수 있다.

## Gradients add at branches

![다운로드 (2).png](/assets/img/cs231n-lecture-4/다운로드 (2).png)

위와 같이 다변수 chain rule에 대해서도 생각해볼 수 있다.

여러 개의 노드와 연결된 경우, gradient를 구할 때도 각 노드로부터 오는 gradient를 합쳐서 구하게 된다.

## A vectorized example

![다운로드 (4).png](/assets/img/cs231n-lecture-4/다운로드 (4).png)

이번에는 벡터화된 예시에 대해 생각해보자. input과 output이 벡터일 때 gradient는 **Jacobian matrix**가 된다.

Jacobian matrix에 대해 간단한 예시를 들자면, 만약 $x=[x_1,x_2,x_3]$이고, $z=[z_1,z_2,z_3]$이라고 한다면 Jacobian matrix는

$$
\begin{bmatrix}
\frac{\partial{z_1}}{\partial{x_1}}&\frac{\partial{z_1}}{\partial{x_2}}&\frac{\partial{z_1}}{\partial{x_3}}\\
\frac{\partial{z_2}}{\partial{x_1}}&\frac{\partial{z_2}}{\partial{x_2}}&\frac{\partial{z_2}}{\partial{x_3}}\\
\frac{\partial{z_3}}{\partial{x_1}}&\frac{\partial{z_3}}{\partial{x_2}}&\frac{\partial{z_3}}{\partial{x_3}}
\end{bmatrix}
$$

가 된다. 따라서, 위의 경우 input vector는 4096 차원이고, output vector도 4096 차원이기 때문에 Jacobian matrix의 크기는 총 4096 x 4096이 된다.

그러나, 실제로 이 전체 Jacobian matrix를 작성할 필요는 없다. 우리는 출력에 대한 x의 영향에 대해서만 알면 된다.

![다운로드 (5).png](/assets/img/cs231n-lecture-4/다운로드 (5).png)

![다운로드 (6).png](/assets/img/cs231n-lecture-4/다운로드 (6).png)

실제 예시는 위와 같다. 어려운 계산식은 없으니 직접 해보자.

## Modularized implementation: forward / backward API

![다운로드 (7).png](/assets/img/cs231n-lecture-4/다운로드 (7).png)

![다운로드 (8).png](/assets/img/cs231n-lecture-4/다운로드 (8).png)

Backpropagation 과정을 실제 모듈화된 방식으로 구현해보자면 위와 같을 것이다.

forward pass에서는 노드의 출력을 계산하는 함수를 구현하고, backward pass에서는 gradient를 계산한다.
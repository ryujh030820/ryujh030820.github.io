---
published: true
title: "[CS231n 11강] Detection and Segmentation"
date: 2024-12-24 23:10:00 +0900
categories: [Lecture, CS231N]
tags: [cs231n, computer vision]
math: true
---
## Semantic Segmentation

지금까지 CS231n 강의는 주로 Image Classification에 대해 다루었다.

오늘은 보다 다양한 Computer Vision Task들을 다뤄볼 예정인데, 우선 **Semantic Segmentation**에 대해 배울 것이다.

![1.png](/assets/img/cs231n-lecture-11/1.png)

**Semantic Segmentation**은 이미지의 픽셀들이 어떤 클래스에 속하는지 예측하는 과정이다.

Classification과 마찬가지로 미리 클래스의 수와 종류를 정해 놓아야 한다.

하지만 Semantic Segmentation은 개별 객체를 구분하지 않는다는 단점이 있다. (위 그림의 소의 예)

Semantic Segmentation 문제에 접근해 볼 수 있는 방법 중 하나는 Classification을 통한 접근이다.

가장 간단하게는 Sliding Window를 적용해볼 수 있다.

![2.png](/assets/img/cs231n-lecture-11/2.png)

입력 이미지를 아주 작은 단위로 쪼개서 해당 영역이 어떤 카테고리에 속하는지를 정하는 방법이다.

하지만 계산 비용이 엄청나게 크기 때문에 이렇게 개별적으로 적용하는 방법은 좋지 않다.

두번째로 제안된 방법은 Fully Convolutional Network를 이용한 방법이다.

![3.png](/assets/img/cs231n-lecture-11/3.png)

위 사진과 같이 FC-Layer가 없고 Convolution Layer로만 구성된 네트워크를 상상해 볼 수 있다.

이 네트워크의 경우 입력 이미지의 Spatial Size(공간 정보)를 유지해야 한다는 특징이 있다.

그래서 비용이 아주 크기 때문에 실제로는 이렇게 생긴 네트워크를 보기는 힘들다.

![4.png](/assets/img/cs231n-lecture-11/4.png)

대신에 이렇게 생긴 네트워크가 대부분이다.

특징 맵을 Downsampling/Upsampling하는 과정을 거치는데, Original Resolution에서는 Conv Layer는 소량만 사용하기 때문에 계산 효율이 더 좋아진다.

이 과정에서 Upsampling이 네트워크 안에서 어떻게 동작하는지 좀 더 자세히 알아보도록 하자.

## Upsampling

Upsampling은 간단히 얘기하면 Downsampling의 역과정이라고 생각하면 된다.

이미지의 사이즈를 늘리게 되면 빈 공간이 생겨날텐데, 이런 빈 공간을 어떻게 채우는지 Issue가 발생한다.

이 강의에서 설명한 방법은 크게 3가지이다.

1. Unpooling
2. Max Unpooling
3. Transpose Convolution
- Unpooling : 빈 공간에 대해 Nearest Neighbor의 경우 인접한 값을 복사하고, Bed of Nails의 경우 다른 곳에는 모두 0을 채워넣는다.
    
    ![5.png](/assets/img/cs231n-lecture-11/5.png)
    
- Max Unpooling: 대부분의 네트워크는 Downsampling/Upsampling의 비율이 대칭적인 경향이 있어, 그 위치로 Upsampling을 하는 과정이다. 공간 정보를 조금은 더 잘 유지할 수 있다는 장점이 있다.
    
    ![6.png](/assets/img/cs231n-lecture-11/6.png)
    
- Transpose Convolution
    
    Stride Convolution과 유사하게 Upsampling에서도 학습 가능한 방법이 있다. 바로 Transpose Convolution이다.
    
    Transpose Convolution은 Convolution 연산과 유사하지만 내적을 수행하지 않고 입력 특징맵에서 값을 하나 선택해 이 스칼라 값을 필터와 곱한다. 그리고 출력 영역에 그 값을 넣는다.
    
    예를 들어, 3x3 transpose convolution, stride 2, pad 1을 한다고 할 때,
    
    ![7.png](/assets/img/cs231n-lecture-11/7.png)
    
    입력값을 가중치라고 생각하고, 필터의 값을 곱해서 output에 채워넣는다.
    
    ![8.png](/assets/img/cs231n-lecture-11/8.png)
    
    만약 겹치는 부분이 있을 경우 그 구간에 있는 값들을 모두 더해주면 된다.
    
    왜 이 연산이 Transpose Convolution라는 이름이 붙었는지 궁금할 수 있다.
    
    이 이름의 유래는 Convolution 연산 자체를 해석해보면 알 수 있다.
    
    필터 : $\vec{x}=(x,y,z)$, 입력 : $\vec{a}=(a,b,c,d)$라 하고 convolution을 진행하면
    
    ![9.png](/assets/img/cs231n-lecture-11/9.png)
    
    이제 Transpose convolution을 진행해보자.
    
    ![10.png](/assets/img/cs231n-lecture-11/10.png)
    
    stride가 1일 때는 둘의 연산이 크게 차이가 없다는 것을 확인할 수 있다.
    
    하지만 stride가 2일 때는 다르다.
    
    ![11.png](/assets/img/cs231n-lecture-11/11.png)
    
    입력 데이터를 Upsampling하기 위해 “0”을 삽입하여 출력 크기가 크게 확장되었다는 느낌을 받을 수 있다.
    
    이래서 Upsampling을 하는데 사용하는 것 같다.
    

## Classification + Localization

**Classification**과 **Localization**은 이미지에서 하나의 물체에 대해 그 물체가 어떤 곳에 속하고, 그 물체가 어디에 위치해 있는지 찾아내는 Task이다.

Localization 문제에서는 이미지 내에서 내가 관심있는 객체가 오직 하나 뿐이라고 가정한다는 점에서 Object Detection과 구분된다.

보통 Image Classification을 위한 네트워크는 최종적으로 하나의 FC layer가 존재하지만 이 Task는 2개의 FC layer를 이용하여 Bounding Box와 Classification을 찾는다.

![13.png](/assets/img/cs231n-lecture-11/13.png)

따라서 학습을 싴리 때 2개의 Loss function을 이용한다.

여기서 2개의 Loss function의 단위가 달라서 gradient 계산이 되는지 의문이 들 수 있다.

하지만 Multitask Loss이기 때문에 네트워크 가중치들의 각각의 미분 값을 계산하므로 상관이 없다.

이러한 방식으로 Human Pose Estimation에도 적용을 할 수 있다.

![14.png](/assets/img/cs231n-lecture-11/14.png)

## Object Detection

Object Detection은 한 장의 이미지에서 다수의 물체를 찾고, 그 물체가 어디 있는지 알아내는 Task이다.

Classification + Localization과 달리 이미지마다 객체의 수가 달라져서 이를 미리 예측할 수 없다. (즉, 더 어려운 Task이다.)

Object Detection은 크게 **2-stage Detector**와 **1-stage Detector**로 나눠질 수 있는데 Classification과 Localization을 단계별로 할 경우 **2-stage Detector**라 하고, 이를 한번에 진행할 경우 **1-stage Detector**라고 한다.

먼저 2-stage Detector에서는 크게 3가지의 알고리즘에 대해 다룰 것이다.

- R-CNN
- Fast R-CNN
- Faster R-CNN

### R-CNN

R-CNN에 대해서 소개하기 전에 먼저 알아야 할 것이 바로 **Selective Search Algorithm**이다.

딥러닝을 쓰지 않고 Rule based로 된 알고리즘으로 물체가 있는 영역을 찾는데 효과적인 알고리즘이다.

![15.png](/assets/img/cs231n-lecture-11/15.png)

Selective Search Algorithm에 대해서 간단히 설명하면 물체의 스케일이나 위치를 모두 고려하여 모든 후보군을 처음에 샅샅이 조사한 다음, Greedy algorithm을 이용해서 여러 영역에서 비슷한 부분을 합치는 식으로 영역을 통합하는 방법이다.

이제 R-CNN에 대해 알아보자.

먼저 방급 언급한 Selective Search Algorithm을 이용해 관심 영역을 추출한다.

![16.png](/assets/img/cs231n-lecture-11/16.png)

다음 CNN에 training을 시키기 위해 input size를 맞춰준다.

![17.png](/assets/img/cs231n-lecture-11/17.png)

CNN을 통과한 다음 SVM을 통해서 classification 과정을 거친다.

![18.png](/assets/img/cs231n-lecture-11/18.png)

다음 Linear Regression을 통해서 Bounding Box도 Loss 값을 계산하고, 두개의 Loss가 최소가 되는 방향으로 training을 시킨다.

![19.png](/assets/img/cs231n-lecture-11/19.png)

지금까지 R-CNN이 어떻게 동작하는지 살펴봤는데, R-CNN은 딥러닝을 이용한 Object Detection의 초창기 모델이라 다음과 같은 단점들이 존재한다.

- 계산 비용이 많이 든다.
- 용량(Memory)도 많이 든다.
- 학습 과정도 느리다. (논문 기준 81시간)
- Test time도 느리다. (한 이미지 당 30초)
- 학습이 되지 않는 Region Proposal이 존재한다.

이러한 단점을 보완하기 위해 나온 알고리즘이 **Fast R-CNN**이다.

### Fast R-CNN

계산 비용이 많이 드는 이유는 각각의 ROI 영역에서 CNN을 수행했기 때문이다.

따라서 Fast R-CNN에서는 전체 Input Image에 대해 한번의 CNN을 수행한다.

![20.png](/assets/img/cs231n-lecture-11/20.png)

그 다음 Feature Map에서 Region Proposal을 찾아낸다.

![21.png](/assets/img/cs231n-lecture-11/21.png)

이때 ROI가 다르기 때문에 Input Size를 맞춰주기 위해 ROI pooling이라는 새로운 기법을 사용하여 Feature map의 Size를 통일한다.

![22.png](/assets/img/cs231n-lecture-11/22.png)

그 다음 Fully Connected Layer를 통해서 Classification에 대한 Loss 값을 계산한다.

![23.png](/assets/img/cs231n-lecture-11/23.png)

마찬가지로 Bounding Box에 대한 Loss 값도 계산한다.

![24.png](/assets/img/cs231n-lecture-11/24.png)

R-CNN과 Fast R-CNN을 시간적인 측면에서 비교를 해보면

![25.png](/assets/img/cs231n-lecture-11/25.png)

한 눈에 보아도 Fast R-CNN이 시간이 훨씬 단축된 것을 볼 수 있다.

그런데, 빨간색 막대를 보면 Test time에서 Region Proposal을 계산하는 시간이 대부분인 것을 알 수 있다.

그렇다면 Region Proposal도 딥러닝으로 수행하면 시간을 단축시키지 않을까?해서 나온 것이 바로 **Faster R-CNN**이다.

### Faster R-CNN

Faster R-CNN은 Feature map을 만드는 CNN과 그 이후 classification + localization 하는데 사용하는 CNN을 공유해서 사용하자는 아이디어에서 출발했다.

따라서 총 4개의 Loss function이 필요하다.

1. RPN classify object / not object (binary classification)
2. RPN regress box coordinates
3. Final classification score (object classes)
4. Final box coordinates

여기서 RPN은 Region Proposal Network를 의미한다

그림으로 Faster R-CNN을 나타내면 다음과 같다.

![26.png](/assets/img/cs231n-lecture-11/26.png)

Faster R-CNN이 얼마나 빨라졌는지 알아보면

![27.png](/assets/img/cs231n-lecture-11/27.png)

네트워크 밖에서 계산하던 Region Proposal의 병목을 제거했기 때문에 다른 네트워크들보다 훨씬 더 빨라진 것을 볼 수 있다.

### Yolo / SSD

이제 2-stage Detector에 대한 얘기가 끝났으니 1-stage Detector에 대한 얘기를 해보자.

1-stage Detector는 크게 2종류로 나눈다.

- YOLO(You Only Look Once)
- SSD(Single Shot MultiBox Detector)

하지만 이 강의에서는 두 종류의 알고리즘을 그냥 한번에 설명하고 넘어간다. (귀차니즘?)

![29.png](/assets/img/cs231n-lecture-11/29.png)

1-stage Detector에 대해 간단히 설명하면 Classification과 Localization을 한 번의 네트워크에서 해결하는 것을 말한다.

ROI 당 별도의 연산을 요구하지 않기 때문에 Region based method에 비해 속도가 빠르다.

## Instance Segmentation

앞서 살펴본 Semantic Segmentation은 개별 객체를 구별할 수 없다는 단점이 있었다.

이 단점을 보완하기 위해 나온 방법이 **Instance Segmentation**이다.

Object Detection 문제처럼 객체별로 여러 객체를 찾고 각각을 구분해준다.

그리고 각 픽셀이 어떤 객체에 속하는지를 전부 다 결정해줘야 한다.

![12.png](/assets/img/cs231n-lecture-11/12.png)

Instance Segmentation과 Object Detection을 결합하여 만들어진 대표적인 알고리즘이 Mask R-CNN이다.

### Mask R-CNN

Mask R-CNN은 Image segmentation을 수행하기 위해 고안된 모델이다.

Mask R-CNN은 기존의 Faster R-CNN이 Object detection 역할을 하도록 하고 각각의 ROI에 mask segmentation을 해주는 작은 FCN(Fully Convolutional Network)를 추가해주었다.

Fast R-CNN과 다른 점은 크게 3가지가 있다.

- Faster R-CNN에 존재하는 “bounding box regression branch”에 병렬로 “object mask prediction branch” 추가
- ROI Pooling 대신 ROI Align 을 사용함
- Mask prediction 과 class prediction 을 decouple 함. (클래스 상관없이 masking)

구조는 다음과 같다.

![28.png](/assets/img/cs231n-lecture-11/28.png)

## 참고

https://taeyoung96.github.io/cs231n/CS231n_11/